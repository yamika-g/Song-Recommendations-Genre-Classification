{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType\n",
    "import os\n",
    "\n",
    "\n",
    "# Define the base paths for attributes and features directories\n",
    "attributes_base_path = \"hdfs:///data/msd/audio/attributes/\"\n",
    "features_base_path = \"hdfs:///data/msd/audio/features/\"\n",
    "\n",
    "# List of attribute directories\n",
    "attribute_directories = [\n",
    "    \"msd-jmir-area-of-moments-all-v1.0\",\n",
    "    \"msd-jmir-lpc-all-v1.0\",\n",
    "    \"msd-jmir-methods-of-moments-all-v1.0\",\n",
    "    \"msd-jmir-mfcc-all-v1.0\",\n",
    "    \"msd-jmir-spectral-all-all-v1.0\",\n",
    "]\n",
    "\n",
    "# Create a dictionary to store attribute schemas\n",
    "attribute_schemas = {}\n",
    "\n",
    "# Iterate through attribute directories and create schemas\n",
    "for attribute_dir in attribute_directories:\n",
    "    # Read the attributes CSV using Spark\n",
    "    attributes_path = os.path.join(attributes_base_path, attribute_dir + \".attributes.csv\")\n",
    "    attributes_df = spark.read.csv(attributes_path, header=False, inferSchema=True)\n",
    "    \n",
    "    # Extract column names from the attributes DataFrame\n",
    "    column_names = attributes_df.select(\"_c0\").rdd.flatMap(lambda x: x).collect()\n",
    "    \n",
    "    # Extract data types from the attributes DataFrame\n",
    "    column_types = attributes_df.select(\"_c1\").rdd.flatMap(lambda x: x).collect()\n",
    "    \n",
    "    # Define a dictionary to map attribute types\n",
    "    type_mapping = {\n",
    "        'real': DoubleType(),\n",
    "        'string': StringType(),\n",
    "        \n",
    "    }\n",
    "    \n",
    "    # Create a list of StructFields for the StructType\n",
    "    struct_fields = [StructField(name, type_mapping.get(data_type, StringType()), True) for name, data_type in zip(column_names, column_types)]\n",
    "    \n",
    "  \n",
    "    \n",
    "    # Create the StructType and store it in the dictionary\n",
    "    attribute_schemas[attribute_dir] = StructType(struct_fields)\n",
    "\n",
    "# Iterate through attribute directories and print their schemas\n",
    "for directory_name, schema in attribute_schemas.items():\n",
    "    print(f\"Schema for {directory_name}:\")\n",
    "    for field in schema.fields:\n",
    "        print(f\"{field.name}: {field.dataType}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "# List of feature directories\n",
    "feature_directories = [\n",
    "    \"msd-jmir-area-of-moments-all-v1.0\",\n",
    "    \"msd-jmir-lpc-all-v1.0\",\n",
    "    \"msd-jmir-methods-of-moments-all-v1.0\",\n",
    "    \"msd-jmir-mfcc-all-v1.0\",\n",
    "    \"msd-jmir-spectral-all-all-v1.0\",]\n",
    "\n",
    "# Create a dictionary to store DataFrames for features\n",
    "feature_dataframes = {}\n",
    "\n",
    "# Iterate through feature directories and read and structure the CSV files\n",
    "for feature_dir in feature_directories:\n",
    "    # Create a StructType for the feature directory based on the corresponding attribute schema\n",
    "    feature_schema = attribute_schemas[feature_dir]\n",
    "\n",
    "    # Construct the HDFS path for the feature files\n",
    "    features_path = f\"hdfs:///data/msd/audio/features/{feature_dir}.csv\"\n",
    "    \n",
    "    # Read all CSV files within the feature directory and concatenate them\n",
    "    feature_files = spark.read.csv(features_path, header=False, inferSchema=True)\n",
    "    # Get the existing column names from the DataFrame\n",
    "    existing_columns = feature_files.columns\n",
    "    \n",
    "    # Rename columns in the DataFrame to match the schema, and retain extra columns\n",
    "    renamed_columns = [col(existing_columns[i]).alias(feature_schema.fieldNames()[i]) for i in range(len(feature_schema))]\n",
    "    \n",
    "    # Select the renamed columns\n",
    "    feature_files = feature_files.select(*renamed_columns)\n",
    "    \n",
    "    # Store the DataFrame in the dictionary with the directory name as the key\n",
    "    feature_dataframes[feature_dir] = feature_files\n",
    "    \n",
    "   \n",
    "\n",
    "# Print schemas for all feature directories\n",
    "for directory_name, schema in attribute_schemas.items():\n",
    "    print(f\"Schema for {directory_name}:\")\n",
    "    for field in schema.fields:\n",
    "        print(f\"{field.name}: {field.dataType}\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "# You now have feature DataFrames with structured data, and you can access them using feature_dataframes[feature_dir]\n",
    "# for any feature directory.\n",
    "\n",
    "# Iterate through feature DataFrames and print the top 5 rows\n",
    "for feature_dir, feature_df in feature_dataframes.items():\n",
    "    print(f\"Top 5 rows for {feature_dir}:\")\n",
    "    feature_df.show(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
